# GBM
Exploration of the Gradient Boosting algorithm for Regression. 
Basically, weak learner, like Trees, are sequentially added to each other, where at each iteration the new base-learner is trained on the error of the whole ensemble.

![GBM](https://github.com/william-tiritilli/GBM/assets/46381506/2f51e0cc-f7d5-4df5-a955-9f2c2b2c4eba)

![gradient-descent-fig-1](https://github.com/william-tiritilli/GBM/assets/46381506/b8a6a90e-ec0c-419b-90d3-55108188dfc9)

This method came a step further with XGBoost, which became the first laureate of many Kaggle competition.
