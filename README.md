# Gradient Boosting Machine (GBM)
Exploration of the Gradient Boosting algorithm for an Car insurance pure premium calculation. 
In this method, short Trees ("weak learners"), are sequentially added to each other, where at each iteration the new base-learner is trained on the error of the whole ensemble.
The results is an a powerful committe of many weak trees that improved at each iteration.

![GBM](https://github.com/william-tiritilli/GBM/assets/46381506/2f51e0cc-f7d5-4df5-a955-9f2c2b2c4eba)

![gradient-descent-fig-1](https://github.com/william-tiritilli/GBM/assets/46381506/b8a6a90e-ec0c-419b-90d3-55108188dfc9)

This method came a step further with XGBoost, which became the first laureate of many Kaggle competition.
