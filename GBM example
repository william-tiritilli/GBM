# XGBoost - Following KA on Schirmacher data

# Libraries
library(dplyr)
library(gbm)
library(xgboost)
library(ggplot2)


# Import of the main file
# Data have been already processed
data<-read.csv("C:\\Users\\William.Tiritilli\\Documents\\Project P\\GBM\\sim-modeling-dataset.csv", )

# glimpse(data)
# dim(data)
# View(data)


# ----sim-data-2------------------------------------------------------------------------------------------------------------------------------------------------------------------
set.seed(54321) # reproductibility

# Create a stratified data partition
train_id <- caret::createDataPartition(
  y = data$clm.count/data$exposure,
  p = 0.8,
  groups = 100
)[[1]]


## ----sim-data-3------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Divide the data in training and test set
data_trn <- data[train_id,]
data_tst <- data[-train_id,]


# Transform variable into factors

data_trn$fuel.type <-as.factor(data_trn$fuel.type)
data_trn$driver.gender <-as.factor(data_trn$driver.gender)
data_trn$yrs.lic  <-as.factor(data_trn$yrs.lic )

# formula
response <- clm.count ~ 
  driver.age + vehicle.age + hp  + fuel.type + driver.gender + yrs.lic +
  offset(log(exposure))

## ----gbm-mtpl-1------------------------------------------------------------------------------------------------------------------------------------------------------------------
set.seed(76539) # reproducibility
fit <- gbm(formula = response,
           data = data_trn,
           distribution = 'poisson',
           # var.monotone = c(0,0,1,0,0,0),
           n.trees = 200,
           interaction.depth = 3,
           n.minobsinnode = 1000,
           shrinkage = 0.1,
           bag.fraction = 0.75,
           cv.folds = 2 # needs to be >1 to use to track
           # the CV error
)
# Track the improvement in the OOB error
oob_evo <- fit$oobag.improve


?gbm

## ----gbm-mtpl-2------------------------------------------------------------------------------------------------------------------------------------------------------------------
ggplot(data.frame(x = 1:200, y = oob_evo), 
       aes(x = x, y = y)) + geom_point() + labs(x = 'Iteration', y = 'OOB error improvement')

# plot error curve
gbm.perf(fit, method = "cv")

# Another way
fit$cv.error
ggplot(data.frame(x = 1:200, y = fit$cv.error), 
       aes(x = x, y = y)) + geom_point() + labs(x = 'Iteration', y = 'cv.error')


# Inspecting the individual trees

## ----gbm-mtpl-3------------------------------------------------------------------------------------------------------------------------------------------------------------------
fit %>% 
  pretty.gbm.tree(i.tree = 1) %>%
  print(digits = 5)

# Does not look that pretty right?!
# Even if this was a nicer representation, single trees are not going to carry much information
# Luckily we know some tools to get a better understanding of the GBM 



# Train a GBM on the MTPL data and obtain some meaningful insights from this model.
# 1. Tune a GBM by tracking the OOB or cross-validation error.
# (Remember that fit$oobag.improve gives you the improvement in the OOB error.)


# Set up a search grid
tgrid <- expand.grid('depth' = c(1,3,5),
                     'ntrees' = NA,
                     'oob_err' = NA)


# Iterate over the search grid
for(i in seq_len(nrow(tgrid))){
  set.seed(76539) # reproducibility
  # Fit a GBM
  fit2 <- gbm(formula = response,
              data = data_trn, distribution = 'poisson',
              #var.monotone = c(0,0,1,0,0,0,0,0,0),
              n.trees = 1000, shrinkage = 0.01,
              interaction.depth = tgrid$depth[i],
              n.minobsinnode = 1000,
              bag.fraction = 0.75, cv.folds = 0
  )
  # Retrieve the optimal number of trees
  opt <- which.max(cumsum(fit$oobag.improve))
  tgrid$ntrees[i] <- opt
  tgrid$oob_err[i] <- cumsum(fit$oobag.improve[1:opt])[opt]
}

library(magrittr)

# Order results on the OOB error
tgrid %<>% arrange(oob_err)

# Fit the optimal GBM
set.seed(76539) # reproducibility
fit_gbm <- gbm(formula = response,
               data = data_trn,
               distribution = 'poisson',
               #var.monotone = c(0,0,1,0,0,0,0,0,0),
               n.trees = tgrid$ntrees[1],
               shrinkage = 0.01,
               interaction.depth = tgrid$depth[1],
               n.minobsinnode = 1000,
               bag.fraction = 0.75,
               cv.folds = 0
)

# Get the built-in feature importance
summary(fit_gbm)

# Need to define this helper function for GBM
pred.fun <- function(object,newdata){
  mean(predict(object, newdata,
               n.trees = object$n.trees))
} 

?pred.fun

str(data)
# PDP for the bonus malus level
fit_gbm %>% 
  partial(pred.fun,
          pred.var = 'yrs.licensed',
          train = data_trn,
          recursive= FALSE) %>% 
  autoplot()

# PDP for the age of the policyholder and power of the car
# WATCH OUT: this will take a very long time to run
fit_gbm %>% 
  partial(pred.fun = pred.fun, 
          pred.var = c('ageph','power'),
          train = mtpl_trn[pdp_ids,],
          recursive= FALSE) %>% 
  autoplot()

?partial

# Extreme Gradient Boosting

## ---- xgb-data-1------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Features go into the data argument 
# (needs to be converted to a matrix)
# The response and offset are specified via 'label' and 'base_margin' 
# in info respectively

data_xgb <- xgb.DMatrix(data = data_trn %>% 
                          select(driver.age, vehicle.age, hp, 
                                 fuel.type, driver.gender, yrs.lic) %>%
                          data.matrix,
                        info = list(
                          'label' = data_trn$clm.count,
                          'base_margin' = log(data_trn$exposure)))

## ---- xgb-data-2------------------------------------------------------------------------------------------------------------------------------------------------------------------

# This result in an xgb.DMatrix object
print(data_xgb)

## ---- xgboost-1-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Train a model
set.seed(86493) # reproducibility
fit <- xgboost(
  data = data_xgb,
  nrounds = 200,
  early_stopping_rounds = 20,
  verbose = FALSE,
  params = list(
    booster = 'gbtree',
    objective  = 'count:poisson',
    eval_metric = 'poisson-nloglik', # for counting 
    eta = 0.1, nthread = 1,
    subsample = 0.75, colsample_bynode = 0.5,
    max_depth = 3, min_child_weight = 1000,
    gamma = 0, lambda = 1, alpha = 1
  )
)

## ---- xgboost-2-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Save xgboost model to a file in binary format
xgb.save(fit, fname = 
           'C:\\Users\\William.Tiritilli\\Documents\\Project P\\GBM\\XGBoost\\xgb.model')
# Load xgboost model from the binary model file
fit <-  xgb.load('C:\\Users\\William.Tiritilli\\Documents\\Project P\\GBM\\XGBoost\\xgb.model')


## ---- xgboost-3-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Transform the test data into a Matrix, format xgb.DMatrix

test_xgb <- data_tst %>%
  select(driver.age, vehicle.age, hp, 
         fuel.type, driver.gender, yrs.lic) %>%
  data.matrix %>% 
  xgb.DMatrix

test_xgb %>% setinfo('base_margin',
                     rep(log(1),
                         nrow(data_tst))
)


## ---- xgboost-4-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Get prediction from the model
preds <- fit %>% predict(
  newdata = test_xgb,
  ntreelimit = fit$niter
) 

preds %>% mean

# The annual claim frequency rate predicted is 16%


## ---- xgb-add-1-------------------------------------------------------------------------------------------------------------------------------------------------------------------

#install.packages('DiagrammeR')
library(DiagrammeR)
#install.packages('glue')


# Inspection of a single tree (here "0" return first value)
xgb.plot.tree(feature_names = colnames(data_xgb),
              model = fit,
              trees = 0)

## ---- xgb-add-2-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Get a compressed view of a XGBoost model:
xgb.plot.multi.trees(model = fit,
                     feature_names = colnames(data_xgb))



## ---- xgb-add-3-------------------------------------------------------------------------------------------------------------------------------------------------------------------

install.packages("Ckmeans.1d.dp")
library(Ckmeans.1d.dp)


# Further built-in interpretations
xgb.ggplot.importance(
  importance_matrix = xgb.importance(
    feature_names = colnames(data_xgb),
    model = fit
  )
)

# EE: 
# It appears that the yrs.lic and driver age  are important
# features in predictig the selling price.

# https://bgreenwell.github.io/pdp/articles/pdp-example-xgboost.html
#library(vip)
# Variable importance plot
#vip(fit, num_features = 5)

library(pdp)  

# c-ICE curves and PDPs for Overall_Qual and Gr_Liv_Area

# What is an ICE curve?
# https://christophm.github.io/interpretable-ml-book/pdp.html

# the below does not work. Because of counting?
x <- data.matrix(subset(data_trn, select = -clm.count))  # training features
p1 <- partial(fit, pred.var = "driver.age", ice = TRUE, center = TRUE, 
              plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "ggplot2", 
              train = x)


# p2 <- partial(ames_xgb, pred.var = "Gr_Liv_Area", ice = TRUE, center = TRUE, 
#               plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "ggplot2",
#               train = x)
# p3 <- partial(ames_xgb, pred.var = c("Overall_Qual", "Gr_Liv_Area"),
#               plot = TRUE, chull = TRUE, plot.engine = "ggplot2", train = x)

# Figure 2
#grid.arrange(p1, p2, p3, ncol = 3)


## ---- xgb-cv-1--------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Built-in cross-validation with xgb.cv

# WATCH OUT: this block can take some time to run
set.seed(86493) # reproducibility
xval <- xgb.cv(data = data_xgb,
               nrounds = 200,
               early_stopping_rounds = 20,
               verbose = FALSE,
               nfold = 5,
               stratified = TRUE,
               params = list(booster = 'gbtree',
                             objective  = 'count:poisson',
                             eval_metric = 'poisson-nloglik',
                             eta = 0.1, nthread = 1,
                             subsample = 0.75, colsample_bynode = 0.5,
                             max_depth = 3, min_child_weight = 1000,
                             gamma = 0, lambda = 1, alpha = 1))



## ---- xgb-cv-2--------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Cross-validation results

xval$evaluation_log %>% print(digits = 5)


## ---- xgb-cv-3--------------------------------------------------------------------------------------------------------------------------------------------------------------------
xval_log <- xval$evaluation_log
xval_log <- as.data.frame(rbind(as.matrix(xval_log[,c(1,2,3)]),as.matrix(xval_log[,c(1,4,5)])))
names(xval_log) <- c('iteration','poisson_nloglik','std')
xval_log$loss <- c(rep('train',nrow(xval_log)/2),rep('test',nrow(xval_log)/2))


## ---- xgb-cv-4--------------------------------------------------------------------------------------------------------------------------------------------------------------------
ggplot(xval_log, aes(x=iteration, y=poisson_nloglik, colour=loss, linetype = loss)) + geom_line(size = 1.3)

ggplot(xval_log[c(150:200,350:400),], aes(x=iteration, y=poisson_nloglik, colour=loss, linetype = loss)) + geom_line(size = 1.5)
# geom_errorbar(aes(ymin=poisson_nloglik-std, ymax=poisson_nloglik+std), width=.1)


# how do you interprete these?




